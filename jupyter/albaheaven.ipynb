{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import time\n",
    "from datetime import datetime\n",
    "from itertools import chain \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경력\n",
      "성별\n",
      "연령\n",
      "학력\n",
      "모집직종\n",
      "고용형태\n",
      "모집인원\n",
      "우대사항\n",
      "기타사항\n",
      "제출서류\n",
      "급여\n",
      "근무기간\n",
      "근무요일\n",
      "근무시간\n",
      "복리후생\n",
      "근무지명\n",
      "사업내용\n",
      "직원수\n",
      "근무지주소\n",
      "동정보\n",
      "위치보기\n",
      "인근지하철\n",
      "인근대학교\n",
      "{'sex': '무관', 'age': (0, 200), 'type_of_pay': '시급', 'pay': '8590', 'worktime': (490, 1020), 'address': '대구 달서구 성서동로 317-1\\xa0303호(장기동)'}\n"
     ]
    }
   ],
   "source": [
    "class AlbaheavenCrawler :\n",
    "    def __init__(self) :\n",
    "        self.base_url = 'http://www.alba.co.kr'\n",
    "        self.detail_source_list = [] #[(url, source)]\n",
    "        \n",
    "    def crawl_from_site(self) :\n",
    "        PAGE_NUM = 1\n",
    "        SLEEP_TIME = 5.0\n",
    "        \n",
    "        detail_url_list = []\n",
    "        with open('heaven_pickle', 'rb') as f :\n",
    "            (detail_url_list, self.detail_source_list) = pickle.load(f)\n",
    "        \n",
    "#         detail_url_list = []\n",
    "#         for page in range(1, 1+PAGE_NUM) :\n",
    "#             time.sleep(SLEEP_TIME)\n",
    "#             source = requests.get(self.base_url + f'/job/Main.asp?page={page}').text\n",
    "#             soup = BeautifulSoup(source, 'html.parser')\n",
    "#             detail_url_list.append(soup.select(\n",
    "#                     '#NormalInfo > table > tbody > tr.firstLine > td.title > span > a.applBtn.blankView')[0]['href'])\n",
    "#             for i in range(3, 10, 2):\n",
    "#             #for i in range(3, 101, 2) :\n",
    "#                 detail_url_list.append(soup.select(\n",
    "#                     f'#NormalInfo > table > tbody > tr:nth-child({i}) > td.title > span > a.applBtn.blankView')[0]['href'])\n",
    "\n",
    "#         for i in detail_url_list :\n",
    "#             self.detail_source_list.append((i, requests.get(self.base_url + i).text))\n",
    "#             time.sleep(SLEEP_TIME)\n",
    "            \n",
    "#         with open('heaven_pickle', 'wb') as f:\n",
    "#             pickle.dump((detail_url_list, self.detail_source_list), f)\n",
    "\n",
    "    def norm_sex(self, sex) :\n",
    "        if sex.count('무관') :\n",
    "            return '무관'\n",
    "        return sex\n",
    "\n",
    "    def norm_age(self, age) :\n",
    "        if age.count('무관') :\n",
    "            return (0, 200) \n",
    "        s_age = age.split()\n",
    "        l = s_age[0]\n",
    "        l = int(l[:l.find('세')])\n",
    "        if s_age[2] == '이전' :\n",
    "            r = 150\n",
    "        else :\n",
    "            r = s_age[2]\n",
    "            r = int(r[:r.find('세')])\n",
    "        return (l, r)\n",
    "\n",
    "#     def norm_pay(self, pay) :\n",
    "#         return pay.replace(',', '')\n",
    "\n",
    "    def norm_worktime(self, worktime) :\n",
    "        if worktime.startswith('시간협의') : \n",
    "            return (None, None)\n",
    "        else :\n",
    "            (l,r) = (worktime[:5], worktime[worktime.find('~')+1:worktime.find('~')+5])\n",
    "            l = int(l[:l.find(':')])*60 + int(l[l.find(':')+1:])\n",
    "            r = int(r[:r.find(':')])*60 + int(r[r.find(':')+1:])\n",
    "            return (l,r)\n",
    "        \n",
    "    def parse_pay_tag(self, tag) :\n",
    "        return (tag.select('.detail-content__pay')[0].i.text, tag.strong.text.replace(',', ''))\n",
    "\n",
    "    def get_text_except_child_from_tag(self, tag) :\n",
    "        return [element for element in tag if isinstance(element, NavigableString)][0].strip()\n",
    "\n",
    "    def get_info_from_source(self, alba_site_number, source) :\n",
    "        cond_selector = [\n",
    "            '#DetailView > div.detail-content > div:nth-child(2) > div > div.detail-content__condition--first > div.detail-content__condition-list',\n",
    "            '#DetailView > div.detail-content > div:nth-child(2) > div > div.detail-content__condition--first > div.detail-content__condition-list.detail-content__list--last',\n",
    "            '#InfoWork > div'\n",
    "        ]\n",
    "        \n",
    "        soup = BeautifulSoup(source, 'html.parser')\n",
    "        \n",
    "        info = {}\n",
    "        for selector in cond_selector:\n",
    "            tags = soup.select(selector)[0].find_all('dl')\n",
    "            for tag in tags :\n",
    "                (dt, dd) = tag.dt.text, tag.dd.text\n",
    "                print(dt)\n",
    "                if dt == '성별' :\n",
    "                    info['sex'] = self.norm_sex(dd)\n",
    "                if dt == '연령' :\n",
    "                    info['age'] = self.norm_age(dd)\n",
    "                if dt == '근무지주소' :\n",
    "                    info['address'] = dd\n",
    "                if dt == '급여' :\n",
    "                    (info['type_of_pay'], info['pay']) = self.parse_pay_tag(tag.dd)\n",
    "                if dt == '근무시간' :\n",
    "                    info['worktime'] = self.norm_worktime(dd)\n",
    "        info['alba_site_name']\n",
    "        return info\n",
    "\n",
    "\n",
    "    def get_info_list(self) :\n",
    "        self.crawl_from_site()\n",
    "        result = []\n",
    "        for i in self.detail_source_list :\n",
    "            result.append(self.get_info_from_source(i[0], i[1]))\n",
    "            try :\n",
    "                result.append(self.get_info_from_source(i[0], i[1]))\n",
    "            except Exception as e:\n",
    "                print('Fail to parse :', i[0])\n",
    "                print(e)\n",
    "                pass\n",
    "        return result\n",
    "\n",
    "bot = AlbaheavenCrawler()\n",
    "bot.crawl_from_site()\n",
    "i = bot.detail_source_list[0]\n",
    "print(bot.get_info_from_source(i[0], i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

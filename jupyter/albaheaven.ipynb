{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import time\n",
    "from datetime import datetime\n",
    "from itertools import chain \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AlbaheavenCrawler :\n",
    "    def __init__(self) :\n",
    "        self.base_url = 'http://www.alba.co.kr'\n",
    "        self.detail_source_list = [] #[(url, source)]\n",
    "        \n",
    "    def crawl_from_site(self) :\n",
    "        PAGE_NUM = 1\n",
    "        SLEEP_TIME = 5.0\n",
    "        \n",
    "        detail_url_list = []\n",
    "        with open('heaven_pickle', 'rb') as f :\n",
    "            (detail_url_list, self.detail_source_list) = pickle.load(f)\n",
    "                   \n",
    "#         for page in range(1, 1+PAGE_NUM) :\n",
    "#             time.sleep(SLEEP_TIME)\n",
    "#             try :\n",
    "#                 source = requests.get(self.base_url + f'/job/Main.asp?page={page}').text\n",
    "#                 soup = BeautifulSoup(source, 'html.parser')\n",
    "#                 detail_url_list.append(soup.select(\n",
    "#                         '#NormalInfo > table > tbody > tr.firstLine > td.title > span > a.applBtn.blankView')[0]['href'])\n",
    "#                 print(f'successfully crawl page {page}')\n",
    "#             except Exception as e:\n",
    "#                 print(f'Fail to crawl page {page} : {e}')\n",
    "\n",
    "#             for i in range(3, 7, 2):\n",
    "#             # for i in range(3, 101, 2):\n",
    "#                 try :\n",
    "#                     detail_url_list.append(soup.select(\n",
    "#                         f'#NormalInfo > table > tbody > tr:nth-child({i}) > td.title > span > a.applBtn.blankView')[0]['href'])\n",
    "#                 except Exception as e:\n",
    "#                     print(f'Fail to crawl page {page} - {i}th content : {e}')\n",
    "\n",
    "#         for i in detail_url_list :\n",
    "#             try :\n",
    "#                 self.detail_source_list.append((i, requests.get(self.base_url + i).text))\n",
    "#             except Exception as e:\n",
    "#                 print(f'Fail to request detail url {i} - {e}')\n",
    "#             time.sleep(SLEEP_TIME)\n",
    "#         with open('heaven_pickle', 'wb') as f:\n",
    "\n",
    "#             pickle.dump((detail_url_list, self.detail_source_list), f)\n",
    "            \n",
    "\n",
    "    def norm_sex(self, sex) :\n",
    "        if sex.count('무관') :\n",
    "            return '무관'\n",
    "        return sex\n",
    "\n",
    "    def norm_age(self, age) :\n",
    "        if age.count('무관') :\n",
    "            return (0, 200) \n",
    "        s_age = age.split()\n",
    "        l = s_age[0]\n",
    "        l = int(l[:l.find('세')])\n",
    "        if s_age[2] == '이전' :\n",
    "            r = 150\n",
    "        else :\n",
    "            r = s_age[2]\n",
    "            r = int(r[:r.find('세')])\n",
    "        return (l, r)\n",
    "\n",
    "    def norm_worktime(self, worktime) :\n",
    "        if worktime.startswith('시간협의') : \n",
    "            return (None, None)\n",
    "        else :\n",
    "            (l,r) = (worktime[:5], worktime[worktime.find('~')+1:worktime.find('~')+5])\n",
    "            l = int(l[:l.find(':')])*60 + int(l[l.find(':')+1:])\n",
    "            r = int(r[:r.find(':')])*60 + int(r[r.find(':')+1:])\n",
    "            return (l,r)\n",
    "        \n",
    "    def parse_pay_tag(self, tag) :\n",
    "        return (tag.i.text, tag.strong.text.replace(',', ''))\n",
    "\n",
    "    def get_text_except_child_from_tag(self, tag) :\n",
    "        return [element for element in tag if isinstance(element, NavigableString)][0].strip()\n",
    "\n",
    "    def get_info_from_source(self, alba_site_number, source) :\n",
    "        cond_selector = [\n",
    "            '#DetailView > div.detail-content > div:nth-child(2) > div > div.detail-content__condition--first > div.detail-content__condition-list',\n",
    "            '#DetailView > div.detail-content > div:nth-child(2) > div > div.detail-content__condition--first > div.detail-content__condition-list.detail-content__list--last',\n",
    "            '#InfoWork > div'\n",
    "        ]\n",
    "        \n",
    "        soup = BeautifulSoup(source, 'html.parser')\n",
    "        \n",
    "        info = {}\n",
    "        \n",
    "        info['title'] = soup.select('#DetailView > div.detail-content > div.detail-content__primary > h2')[0].text\n",
    "        for selector in cond_selector:\n",
    "            tags = soup.select(selector)[0].find_all('dl')\n",
    "            for tag in tags :\n",
    "                (dt, dd) = tag.dt.text, tag.dd.text\n",
    "                if dt == '성별' :\n",
    "                    info['sex'] = self.norm_sex(dd)\n",
    "                if dt == '연령' :\n",
    "                    info['age'] = self.norm_age(dd)\n",
    "                if dt == '근무지주소' :\n",
    "                    info['address'] = dd\n",
    "                if dt == '급여' :\n",
    "                    (info['type_of_pay'], info['pay']) = self.parse_pay_tag(tag.dd)\n",
    "                if dt == '근무시간' :\n",
    "                    info['worktime'] = self.norm_worktime(dd)\n",
    "        info['alba_site_name'] = '알바천국'\n",
    "        info['alba_site_number'] = alba_site_number[alba_site_number.find('adid=')+5:alba_site_number.find('&list')]\n",
    "        return info\n",
    "\n",
    "\n",
    "    def get_info_list(self) :\n",
    "        self.crawl_from_site()\n",
    "        result = []\n",
    "        for i in self.detail_source_list :\n",
    "            try :\n",
    "                result.append(self.get_info_from_source(i[0], i[1]))\n",
    "            except Exception as e:\n",
    "                print(f'Fail to parse {i[1]} : {e}')\n",
    "                pass\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': '(주급가능)전기계량기 조립 및 포장 검수 사원모집합니다.',\n",
       "  'sex': '무관',\n",
       "  'age': (0, 200),\n",
       "  'type_of_pay': '시급',\n",
       "  'pay': '8590',\n",
       "  'worktime': (490, 1020),\n",
       "  'address': '대구 달서구 성서동로 317-1\\xa0303호(장기동)',\n",
       "  'alba_site_name': '알바천국',\n",
       "  'alba_site_number': '10149394'},\n",
       " {'title': '하남시디플러스 배달대행 기사모집 [하남최대규모][기본3500원][일당근무o]',\n",
       "  'sex': '무관',\n",
       "  'age': (0, 200),\n",
       "  'type_of_pay': '건별',\n",
       "  'pay': '3500',\n",
       "  'worktime': (600, 120),\n",
       "  'address': '경기 하남시 신장1로 17\\xa0(신장동)',\n",
       "  'alba_site_name': '알바천국',\n",
       "  'alba_site_number': '10145621'},\n",
       " {'title': '서면 롯데백화점 타르트 제조.판매 단기 알바 구해요',\n",
       "  'sex': '무관',\n",
       "  'age': (0, 200),\n",
       "  'type_of_pay': '시급',\n",
       "  'pay': '10300',\n",
       "  'worktime': (540, 1200),\n",
       "  'address': '부산 부산진구 중앙대로692번길 45-7\\xa0프띠르',\n",
       "  'alba_site_name': '알바천국',\n",
       "  'alba_site_number': '10147690'},\n",
       " {'title': '[주5일]하루5시간 시급15000원 풀타임도가능  합정본점',\n",
       "  'sex': '무관',\n",
       "  'age': (21, 65),\n",
       "  'type_of_pay': '시급',\n",
       "  'pay': '15000',\n",
       "  'worktime': (600, 900),\n",
       "  'address': '서울 마포구 양화로 26\\xa0(KCC엠파이어리버)',\n",
       "  'alba_site_name': '알바천국',\n",
       "  'alba_site_number': '10149108'},\n",
       " {'title': '+건설6명급구+단기목돈마련日12만원-18만.월350만. 잔업풀.숙식지원.',\n",
       "  'sex': '무관',\n",
       "  'age': (0, 200),\n",
       "  'type_of_pay': '월급',\n",
       "  'pay': '3000000',\n",
       "  'worktime': (420, 1023),\n",
       "  'address': '경기 이천시 부발읍 경충대로 2091\\xa0(에스케이하이닉스)',\n",
       "  'alba_site_name': '알바천국',\n",
       "  'alba_site_number': '10146073'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot = AlbaheavenCrawler()\n",
    "bot.get_info_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

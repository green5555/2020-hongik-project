{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbamonCrawler :\n",
    "    def __init__(self) :\n",
    "        self.base_url = 'https://www.albamon.com/'\n",
    "        self.detail_source_list = [] #[(url, source)]\n",
    "        \n",
    "    def norm_age(self, age) :\n",
    "        if age == \"무관\" :\n",
    "            return (None, None)\n",
    "        l = age.split()[0]\n",
    "        l = int(l[:l.find('세')])\n",
    "        r = age.split()[2]\n",
    "        r = int(r[:r.find('세')])\n",
    "        return (l, r)\n",
    "\n",
    "    def norm_pay(self, pay) :\n",
    "        return pay[:-1].replace(',', '')\n",
    "\n",
    "    def norm_worktime(self, worktime) :\n",
    "        if worktime.startswith('시간협의') : \n",
    "            return (None, None)\n",
    "        else :\n",
    "            (l, r) = (worktime[:5], worktime[6:11])\n",
    "            l = int(l[:l.find(':')])*60 + int(l[l.find(':')+1:])\n",
    "            r = int(r[:r.find(':')])*60 + int(r[r.find(':')+1:])\n",
    "            return (l,r)\n",
    "\n",
    "    def get_info_from_source(self, alba_site_number, source) :\n",
    "        '''\n",
    "        source : 주소 url에 request를 날린 text\n",
    "        return : dict\n",
    "                {\n",
    "                'sex' : str, '남자', '여자', '무관'\n",
    "                'age' : tuple(int, int), (낮은 나이, 높은 나이), 없으면(None, None)\n",
    "                'address' : str\n",
    "                'pay' : int\n",
    "                'type_of_pay' : str, '월급', '일급', etc\n",
    "                'worktime' : tuple(int, int), (시작 시간의 분, 끝 시간의 분), 없으면(None, None)\n",
    "                'alba_site_name' : str\n",
    "                'alba_site_number' : str\n",
    "                }\n",
    "        '''\n",
    "        soup = BeautifulSoup(source, 'html.parser')\n",
    "        condition_selector = '#allcontent > div.viewContent.viewRecruitType > div.viewTypeFullWidth > div.conditionInfo.verticalLine > div.column.column_620.infoBox > div.recruitCondition > div > table > tbody'\n",
    "        info = {}\n",
    "        for tr_tag in soup.select(condition_selector)[0].find_all(\"tr\") :\n",
    "            if tr_tag.find(\"th\").text == \"성별\" :\n",
    "                info['sex'] = tr_tag.find(\"span\").text\n",
    "            if tr_tag.find(\"th\").text == \"연령\" :\n",
    "                info['age'] = self.norm_age(tr_tag.find(\"span\").text)\n",
    "        info['address']= soup.select(\n",
    "            f'div.viewContent.viewRecruitType > div.viweTab > div.tabItem_workArea > div.workAddr > span')[0].text\n",
    "        info['pay']= self.norm_pay(soup.select(\n",
    "            f'div.workCondition > div.viewTable > table > tbody > tr:nth-child(1) > td > div.payInfoBox > span.monthPay')[0].text)\n",
    "        info['type_of_pay']= soup.select(\n",
    "            f'div.workCondition > div.viewTable > table > tbody > tr:nth-child(1) > td > div.payInfoBox > span.textPoint > strong')[0].text\n",
    "        info['worktime']= self.norm_worktime(soup.select(\n",
    "        f'div.workCondition > div.viewTable > table > tbody > tr:nth-child(4) > td > span')[0].text.strip())\n",
    "        info['alba_site_name'] = 'albamon'\n",
    "        info['alba_site_number'] = alba_site_number\n",
    "        return info\n",
    "    \n",
    "    def crawl_from_site(self) :\n",
    "        PAGE_NUM = 1\n",
    "        SLEEP_TIME = 5.0\n",
    "\n",
    "        #PAGE_NUM만큼 URL 리스트를 뽑는다.\n",
    "        #알바몬은 한 페이지 당 20개\n",
    "        detail_url_list = []\n",
    "        for page in range(1, 1+PAGE_NUM) :\n",
    "            time.sleep(SLEEP_TIME)\n",
    "            source = requests.get(self.base_url + f'/list/gi/mon_gi_tot_list.asp?page={page}').text\n",
    "            soup = BeautifulSoup(source, 'html.parser')\n",
    "        \n",
    "            for i in range(5) :\n",
    "            #for i in range(20) :\n",
    "                detail_url_list.append(soup.select(f'td.subject > div.subWrap > p.cName > a')[i]['href'])\n",
    "\n",
    "        #URL 리스트들에서 소스를 얻어낸다\n",
    "        for i in detail_url_list :\n",
    "            self.detail_source_list.append((i, requests.get(self.base_url + i).text))\n",
    "            time.sleep(SLEEP_TIME)\n",
    "            \n",
    "        #소스들을 피클로 저장\n",
    "        save_pkl_file_name = 'albamon_source_' + str(datetime.now().isoformat(sep='_')).replace(':','-') +'.pkl'\n",
    "        with open(save_pkl_file_name, 'wb') as f:\n",
    "            pickle.dump(self.detail_source_list, f)\n",
    "            \n",
    "    def get_alba_site_number_form_url(self, url) :\n",
    "        end_pos = url.find('&mj_stat')\n",
    "        return url[26:end_pos]\n",
    "        \n",
    "    def get_info_list(self) :\n",
    "        # open_pkl_file_name = 'C:\\\\Users\\\\Green\\\\Desktop\\\\2020_hongik_project\\\\app\\\\midterm_alba\\\\crawler\\\\albamon_source_2020-05-21_14-09-07.593744.pkl'\n",
    "        # with open(open_pkl_file_name, 'rb') as f:\n",
    "        #     self.detail_source_list = pickle.load(f)\n",
    "        self.crawl_from_site()\n",
    "        result = []\n",
    "        for i in self.detail_source_list :\n",
    "            result.append(self.get_info_from_source(self.get_alba_site_number_form_url(i[0]), i[1]))\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = AlbamonCrawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-657a6418746e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_info_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-a9b21fe23ca1>\u001b[0m in \u001b[0;36mget_info_list\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;31m# with open(open_pkl_file_name, 'rb') as f:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;31m#     self.detail_source_list = pickle.load(f)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawl_from_site\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetail_source_list\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-a9b21fe23ca1>\u001b[0m in \u001b[0;36mcrawl_from_site\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[1;31m#for i in range(20) :\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m                 \u001b[0mdetail_url_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'td.subject > div.subWrap > p.cName > a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m#URL 리스트들에서 소스를 얻어낸다\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "bot.get_info_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
